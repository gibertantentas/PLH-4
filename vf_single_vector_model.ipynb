{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYuSELTz92sA",
        "outputId": "07daa8ad-0098-4a73-b17c-5d22d1066dbe"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "iWyWyt5X7GRK"
      },
      "source": [
        "# Semantic Text Similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-28T09:57:01.673450Z",
          "start_time": "2024-05-28T09:57:01.056869Z"
        },
        "id": "TmWpktp29xis"
      },
      "outputs": [],
      "source": [
        "# Requisites\n",
        "from gensim.models import TfidfModel\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.corpora import Dictionary\n",
        "import numpy as np\n",
        "from scipy import spatial\n",
        "from typing import Tuple, List\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-28T09:57:01.675833Z",
          "start_time": "2024-05-28T09:57:01.674277Z"
        },
        "id": "7LBFDMHi9xit"
      },
      "outputs": [],
      "source": [
        "# Load stopwords in spanish\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "#STOPWORDS_ES = {\"yo\", \"tú\", \"él\", \"ella\", \"nosotros\", \"vosotros\", \"ellos\", \"de\", \"a\", }\n",
        "STOPWORDS_CAT = {\"jo\", \"tu\", \"ell\", \"ella\", \"nosaltres\", \"vosaltres\", \"ells\", \"elles\", \"de\", \"a\", }\n",
        "#STOPWORDS_CAT = stopwords.words(\"catalan\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-28T09:57:01.677989Z",
          "start_time": "2024-05-28T09:57:01.676426Z"
        },
        "id": "6RhXHDNN7GRO"
      },
      "outputs": [],
      "source": [
        "# Define preprocessing\n",
        "def preprocess(sentence: str) -> List[str]:\n",
        "    preprocessed = simple_preprocess(sentence)\n",
        "    preprocessed = [token for token in preprocessed if token not in STOPWORDS_CAT]\n",
        "    return preprocessed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "lANXoJjm7GRO"
      },
      "source": [
        "# Load Vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-28T09:57:01.680368Z",
          "start_time": "2024-05-28T09:57:01.678937Z"
        },
        "id": "Sf5afpx6afWh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9eee778f-182f-4da7-ec1a-96d4d7020d33"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# WORD 2 VEC\\nWORD_EMBEDDING_FILE = \\'/content/drive/MyDrive/2n/PLH/PLH-4/cc.ca.300.bin\\'\\n# Load with gensim\\nfrom gensim.models import fasttext\\nwv_model = fasttext.load_facebook_vectors(WORD_EMBEDDING_FILE)\\n\\n#WORD_EMBEDDING_FILE = load_dataset(\"projecte-aina/catalan_general_crawling\")'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "'''# WORD 2 VEC\n",
        "WORD_EMBEDDING_FILE = '/content/drive/MyDrive/2n/PLH/PLH-4/cc.ca.300.bin'\n",
        "# Load with gensim\n",
        "from gensim.models import fasttext\n",
        "wv_model = fasttext.load_facebook_vectors(WORD_EMBEDDING_FILE)\n",
        "\n",
        "#WORD_EMBEDDING_FILE = load_dataset(\"projecte-aina/catalan_general_crawling\")'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ROBERTA\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"projecte-aina/roberta-base-ca-v2\")\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"projecte-aina/roberta-base-ca-v2\")\n",
        "\n",
        "import torch\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "# Cargar el modelo y el tokenizer de Roberta\n",
        "#tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "#model = RobertaModel.from_pretrained('roberta-base')\n",
        "\n",
        "# Extraer los embeddings del vocabulario\n",
        "def get_word_embeddings(model, tokenizer):\n",
        "    embeddings = model.get_input_embeddings().weight.detach().cpu().numpy()\n",
        "    vocab = tokenizer.get_vocab()\n",
        "    word_embeddings = {word: embeddings[idx] for word, idx in vocab.items()}\n",
        "    return word_embeddings\n",
        "\n",
        "# Obtener los embeddings\n",
        "word_embeddings = get_word_embeddings(model, tokenizer)\n",
        "\n",
        "# Opcional: Reducir la dimensionalidad de 768 a 300\n",
        "def reduce_dimensionality(word_embeddings, n_components=300):\n",
        "    words = list(word_embeddings.keys())\n",
        "    embeddings = np.array(list(word_embeddings.values()))\n",
        "    pca = PCA(n_components=n_components)\n",
        "    reduced_embeddings = pca.fit_transform(embeddings)\n",
        "    reduced_word_embeddings = {word: reduced_embeddings[idx] for idx, word in enumerate(words)}\n",
        "    return reduced_word_embeddings\n",
        "reduced_word_embeddings = reduce_dimensionality(word_embeddings, n_components=300)\n",
        "\n"
      ],
      "metadata": {
        "id": "S0hi9xQ9crMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Load the Roberta model and tokenizer\n",
        "roberta_tokenizer = AutoTokenizer.from_pretrained(\"projecte-aina/roberta-base-ca-v2\")\n",
        "roberta_model = AutoModelForMaskedLM.from_pretrained(\"projecte-aina/roberta-base-ca-v2\").roberta\n",
        "\n",
        "def get_roberta_embeddings(roberta_model, roberta_tokenizer):\n",
        "    vocab = roberta_tokenizer.get_vocab()\n",
        "    embeddings = {}\n",
        "    for word, idx in vocab.items():\n",
        "        inputs = roberta_tokenizer(word, return_tensors='pt', truncation=True, padding=True)\n",
        "        with torch.no_grad():\n",
        "            outputs = roberta_model(**inputs)\n",
        "        embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
        "        embeddings[word] = embedding\n",
        "    return embeddings\n",
        "\n",
        "word_embeddings = get_roberta_embeddings(roberta_model, roberta_tokenizer)\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def reduce_dimensionality(word_embeddings, n_components=300):\n",
        "    words = list(word_embeddings.keys())\n",
        "    embeddings = np.array(list(word_embeddings.values()))\n",
        "    pca = PCA(n_components=n_components)\n",
        "    reduced_embeddings = pca.fit_transform(embeddings)\n",
        "    reduced_word_embeddings = {word: reduced_embeddings[idx] for idx, word in enumerate(words)}\n",
        "    return reduced_word_embeddings\n",
        "\n",
        "reduced_word_embeddings = reduce_dimensionality(word_embeddings, n_components=300)\n",
        "'''"
      ],
      "metadata": {
        "id": "Wr4ErZdPUpYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomKeyedVectors:\n",
        "    def __init__(self, word_embeddings):\n",
        "        self.word_embeddings = word_embeddings\n",
        "\n",
        "    def __getitem__(self, word):\n",
        "        return self.word_embeddings[word]\n",
        "\n",
        "    def __contains__(self, word):\n",
        "        return word in self.word_embeddings\n",
        "\n",
        "# Crear el objeto wv_model\n",
        "wv_model = CustomKeyedVectors(reduced_word_embeddings)\n"
      ],
      "metadata": {
        "id": "uzx8s2pJUt_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0JyxLSc9xiu"
      },
      "outputs": [],
      "source": [
        "'''# SPACY\n",
        "!python -m spacy download ca_core_news_md\n",
        "import spacy\n",
        "model = spacy.load(\"ca_core_news_md\")\n",
        "wv_model = model.vocab.vectors\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ks8KZhNP9xiu"
      },
      "outputs": [],
      "source": [
        "# ONE HOT ENCODING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-28T09:58:57.079699Z",
          "start_time": "2024-05-28T09:58:57.072399Z"
        },
        "id": "dUUuDMup7GRO"
      },
      "outputs": [],
      "source": [
        "# If you want, you can use mmaps\n",
        "USE_MMAP = False\n",
        "if USE_MMAP:\n",
        "    from gensim.models.fasttext import FastTextKeyedVectors\n",
        "    MMAP_PATH = 'cc.ca.gensim.bin'\n",
        "    # wv_model.save(MMAP_PATH)\n",
        "    wv_model = FastTextKeyedVectors.load(MMAP_PATH, mmap='r')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-28T09:58:57.178850Z",
          "start_time": "2024-05-28T09:58:57.081387Z"
        },
        "id": "7o_989Kp7GRP"
      },
      "outputs": [],
      "source": [
        "# Sample data\n",
        "input_pairs_example = [\n",
        "    ('Me gusta el futbol', 'Disfruto viendo partidos de futbol', 4),\n",
        "    ('El cielo está despejado', 'Hace un día bonito', 4.5),\n",
        "    ('Me encanta viajar', 'Explorar nuevos lugares es una pasión', 3.5),\n",
        "    ('Prefiero el verano', 'No me gusta el frío del invierno', 2.5),\n",
        "    ('Tengo hambre', '¿Qué hay para cenar?', 2),\n",
        "    ('La música me relaja', 'Escuchar música es una terapia', 3),\n",
        "    ('El libro es emocionante', 'No puedo dejar de leerlo', 4),\n",
        "    ('Me gusta la pizza', 'Es mi comida favorita', 4.5),\n",
        "    ('Estoy cansado', 'Necesito hacer una siesta', 1.5),\n",
        "    ('Hoy hace mucho calor', 'Es un día sofocante', 3.5)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-28T09:59:02.451116Z",
          "start_time": "2024-05-28T09:58:57.179728Z"
        },
        "id": "BhTzvKLn7GRP"
      },
      "outputs": [],
      "source": [
        "# Real data\n",
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "#dataset = load_dataset(\"PlanTL-GOB-ES/sts-es\")\n",
        "dataset = load_dataset(\"projecte-aina/sts-ca\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLLF86yg9xiv"
      },
      "outputs": [],
      "source": [
        "'''dataset['train'] = dataset['train'].select(range(len(dataset['train']) // 200))\n",
        "dataset['validation'] = dataset['validation'].select(range(len(dataset['validation']) // 200))\n",
        "dataset['test'] = dataset['test'].select(range(len(dataset['test']) // 200))\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-28T09:59:02.460595Z",
          "start_time": "2024-05-28T09:59:02.451912Z"
        },
        "id": "vQDyEimj7GRP"
      },
      "outputs": [],
      "source": [
        "input_pairs = [(e[\"sentence1\"], e[\"sentence2\"], e[\"label\"], ) for e in dataset[\"train\"].to_list()]\n",
        "input_pairs_val = [(e[\"sentence1\"], e[\"sentence2\"], e[\"label\"], ) for e in dataset[\"validation\"].to_list()]\n",
        "input_pairs_test = [(e[\"sentence1\"], e[\"sentence2\"], e[\"label\"], ) for e in dataset[\"test\"].to_list()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-28T09:59:02.561477Z",
          "start_time": "2024-05-28T09:59:02.462797Z"
        },
        "id": "rMqnONbQ7GRP"
      },
      "outputs": [],
      "source": [
        "all_input_pairs = input_pairs + input_pairs_val + input_pairs_test\n",
        "# Preprocesamiento de las oraciones y creación del diccionario\n",
        "sentences_1_preproc = [simple_preprocess(sentence_1) for sentence_1, _, _ in all_input_pairs]\n",
        "sentences_2_preproc = [simple_preprocess(sentence_2) for _, sentence_2, _ in all_input_pairs]\n",
        "sentence_pairs = list(zip(sentences_1_preproc, sentences_2_preproc))\n",
        "# Versión aplanada para poder entrenar el modelo\n",
        "sentences_pairs_flattened = sentences_1_preproc + sentences_2_preproc\n",
        "diccionario = Dictionary(sentences_pairs_flattened)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-28T09:59:02.563822Z",
          "start_time": "2024-05-28T09:59:02.562060Z"
        },
        "id": "kyt-gWng7GRQ"
      },
      "outputs": [],
      "source": [
        "print(sentence_pairs[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-28T09:59:02.604166Z",
          "start_time": "2024-05-28T09:59:02.564663Z"
        },
        "id": "0HYg0SVy7GRQ"
      },
      "outputs": [],
      "source": [
        "# Cálculo de los pesos TF-IDF para las oraciones pre-procesadas\n",
        "corpus = [diccionario.doc2bow(sent) for sent in sentences_pairs_flattened]\n",
        "modelo_tfidf = TfidfModel(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Per word 2 vec\n",
        "def map_tf_idf(sentence_preproc: List[str], dictionary: Dictionary, tf_idf_model: TfidfModel) -> Tuple[List[np.ndarray], List[float]]:\n",
        "    bow = dictionary.doc2bow(sentence_preproc)\n",
        "    tf_idf = tf_idf_model[bow]\n",
        "    vectors, weights = [], []\n",
        "    for word_index, weight in tf_idf:\n",
        "        word = dictionary.get(word_index)\n",
        "        if word in wv_model:\n",
        "            vectors.append(wv_model[word])\n",
        "            weights.append(weight)\n",
        "    return vectors, weights\n",
        "\n",
        "def map_pairs_vells(\n",
        "        sentence_pairs: List[Tuple[str, str, float]],\n",
        "        dictionary: Dictionary = None,\n",
        "        tf_idf_model: TfidfModel = None,\n",
        ") -> List[Tuple[Tuple[np.ndarray, np.ndarray], float]]:\n",
        "    # Mapeo de los pares de oraciones a pares de vectores\n",
        "    pares_vectores = []\n",
        "    for i, (sentence_1, sentence_2, similitud) in enumerate(sentence_pairs):\n",
        "        sentence_1_preproc = preprocess(sentence_1)\n",
        "        sentence_2_preproc = preprocess(sentence_2)\n",
        "        # Si usamos TF-IDF\n",
        "        if tf_idf_model is not None:\n",
        "            # Cálculo del promedio ponderado por TF-IDF de los word embeddings\n",
        "            vectors1, weights1 = map_tf_idf(sentence_1_preproc, dictionary=dictionary, tf_idf_model=tf_idf_model, )\n",
        "            vectors2, weights2 = map_tf_idf(sentence_2_preproc, dictionary=dictionary, tf_idf_model=tf_idf_model, )\n",
        "            vector1 = np.average(vectors1, weights=weights1, axis=0, )\n",
        "            vector2 = np.average(vectors2, weights=weights2, axis=0, )\n",
        "        else:\n",
        "            # Cálculo del promedio de los word embeddings\n",
        "            vectors1 = [wv_model[word] for word in sentence_1_preproc if word in wv_model]\n",
        "            vectors2 = [wv_model[word] for word in sentence_2_preproc if word in wv_model]\n",
        "            vector1 = np.mean(vectors1, axis=0)\n",
        "            vector2 = np.mean(vectors2, axis=0)\n",
        "        # Añadir a la lista\n",
        "        pares_vectores.append(((vector1, vector2), similitud))\n",
        "    return pares_vectores"
      ],
      "metadata": {
        "id": "64DgiLBOeAN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-28T09:59:02.608679Z",
          "start_time": "2024-05-28T09:59:02.604885Z"
        },
        "id": "aiXFyd9E7GRQ"
      },
      "outputs": [],
      "source": [
        "def map_pairs(sentence_pairs, dictionary=None, tf_idf_model=None):\n",
        "    pares_vectores = []\n",
        "    for i, (sentence_1, sentence_2, similitud) in enumerate(sentence_pairs):\n",
        "        sentence_1_preproc = preprocess(sentence_1)\n",
        "        sentence_2_preproc = preprocess(sentence_2)\n",
        "        if tf_idf_model is not None:\n",
        "            vectors1, weights1 = map_tf_idf(sentence_1_preproc, dictionary=dictionary, tf_idf_model=tf_idf_model)\n",
        "            vectors2, weights2 = map_tf_idf(sentence_2_preproc, dictionary=dictionary, tf_idf_model=tf_idf_model)\n",
        "            if weights1 and weights2 and sum(weights1) != 0 and sum(weights2) != 0:\n",
        "                vector1 = np.average(vectors1, weights=weights1, axis=0)\n",
        "                vector2 = np.average(vectors2, weights=weights2, axis=0)\n",
        "            else:\n",
        "                vector1 = np.mean(vectors1, axis=0) if vectors1 else np.zeros(300)\n",
        "                vector2 = np.mean(vectors2, axis=0) if vectors2 else np.zeros(300)\n",
        "        else:\n",
        "            vectors1 = [wv_model[word] for word in sentence_1_preproc if word in wv_model]\n",
        "            vectors2 = [wv_model[word] for word in sentence_2_preproc if word in wv_model]\n",
        "            vector1 = np.mean(vectors1, axis=0) if vectors1 else np.zeros(300)\n",
        "            vector2 = np.mean(vectors2, axis=0) if vectors2 else np.zeros(300)\n",
        "        pares_vectores.append(((vector1, vector2), similitud))\n",
        "    return pares_vectores\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-28T09:59:03.411990Z",
          "start_time": "2024-05-28T09:59:02.609287Z"
        },
        "id": "VArkNJ0i9xiv"
      },
      "outputs": [],
      "source": [
        "# Imprimir los pares de vectores y la puntuación de similitud asociada\n",
        "mapped = map_pairs(input_pairs, tf_idf_model=modelo_tfidf, dictionary=diccionario, )\n",
        "# Imprimir los pares de vectores y la puntuación de similitud asociada\n",
        "mapped_train = map_pairs(input_pairs,  tf_idf_model=modelo_tfidf, dictionary=diccionario, )\n",
        "mapped_val = map_pairs(input_pairs_val, tf_idf_model=modelo_tfidf, dictionary=diccionario, )\n",
        "mapped_test = map_pairs(input_pairs_test, tf_idf_model=modelo_tfidf, dictionary=diccionario, )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-28T09:59:03.415028Z",
          "start_time": "2024-05-28T09:59:03.412652Z"
        },
        "id": "MgiwMu8x7GRR"
      },
      "outputs": [],
      "source": [
        "for vectors, similitud in mapped[:5]:\n",
        "    print(f\"Pares de vectores: {vectors[0].shape}, {vectors[1].shape}\")\n",
        "    print(f\"Puntuación de similitud: {similitud}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-28T12:46:39.753349Z",
          "start_time": "2024-05-28T12:46:39.746306Z"
        },
        "id": "G0_GXuUJ7GRR"
      },
      "outputs": [],
      "source": [
        "# Define the Model\n",
        "import tensorflow as tf\n",
        "\n",
        "def build_and_compile_model(embedding_size: int = 300, learning_rate: float = 1e-3) -> tf.keras.Model:\n",
        "    # Capa de entrada para los pares de vectores\n",
        "    input_1 = tf.keras.Input(shape=(embedding_size,))\n",
        "    input_2 = tf.keras.Input(shape=(embedding_size,))\n",
        "\n",
        "    # Hidden layer\n",
        "    first_projection = tf.keras.layers.Dense(\n",
        "        embedding_size,\n",
        "        kernel_initializer=tf.keras.initializers.Identity(),\n",
        "        bias_initializer=tf.keras.initializers.Zeros(),\n",
        "    )\n",
        "    projected_1 = first_projection(input_1)\n",
        "    projected_2 = first_projection(input_2)\n",
        "\n",
        "    # Compute the cosine distance using a Lambda layer\n",
        "    def cosine_distance(x):\n",
        "        x1, x2 = x\n",
        "        x1_normalized = tf.keras.backend.l2_normalize(x1, axis=1)\n",
        "        x2_normalized = tf.keras.backend.l2_normalize(x2, axis=1)\n",
        "        return 2.5 * (1.0 + tf.reduce_sum(x1_normalized * x2_normalized, axis=1))\n",
        "\n",
        "    output = tf.keras.layers.Lambda(cosine_distance)([projected_1, projected_2])\n",
        "    # Define output\n",
        "    model = tf.keras.Model(inputs=[input_1, input_2], outputs=output)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(loss='mean_squared_error',\n",
        "                  optimizer=tf.keras.optimizers.Adamax(learning_rate))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-28T12:46:39.936328Z",
          "start_time": "2024-05-28T12:46:39.931754Z"
        },
        "id": "T54-FTuV9xiw"
      },
      "outputs": [],
      "source": [
        "def build_and_compile_model_better(embedding_size: int = 300, learning_rate: float = 1e-3) -> tf.keras.Model:\n",
        "    # Capa de entrada para los pares de vectores\n",
        "    input_1 = tf.keras.Input(shape=(embedding_size,))\n",
        "    input_2 = tf.keras.Input(shape=(embedding_size,))\n",
        "\n",
        "    # Hidden layer\n",
        "    first_projection = tf.keras.layers.Dense(\n",
        "        embedding_size,\n",
        "        kernel_initializer=tf.keras.initializers.Identity(),\n",
        "        bias_initializer=tf.keras.initializers.Zeros(),\n",
        "    )\n",
        "    projected_1 =  first_projection(input_1)\n",
        "    projected_2 = first_projection(input_2)\n",
        "\n",
        "    # Compute the cosine distance using a Lambda layer\n",
        "    def normalized_product(x):\n",
        "        x1, x2 = x\n",
        "        x1_normalized = tf.keras.backend.l2_normalize(x1, axis=1)\n",
        "        x2_normalized = tf.keras.backend.l2_normalize(x2, axis=1)\n",
        "        return x1_normalized * x2_normalized\n",
        "\n",
        "    output = tf.keras.layers.Lambda(normalized_product)([projected_1, projected_2])\n",
        "    output = tf.keras.layers.Dropout(0.1)(output)\n",
        "    output = tf.keras.layers.Dense(\n",
        "        16,\n",
        "        activation=\"relu\",\n",
        "    )(output)\n",
        "    output = tf.keras.layers.Dropout(0.2)(output)\n",
        "    output = tf.keras.layers.Dense(\n",
        "        1,\n",
        "        activation=\"sigmoid\",\n",
        "    )(output)\n",
        "\n",
        "    output = tf.keras.layers.Lambda(lambda x: x * 5)(output)\n",
        "\n",
        "    # Define output\n",
        "    model = tf.keras.Model(inputs=[input_1, input_2], outputs=output)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(loss='mean_squared_error',\n",
        "                  optimizer=tf.keras.optimizers.Adam(learning_rate))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-28T12:46:40.098561Z",
          "start_time": "2024-05-28T12:46:40.095327Z"
        },
        "id": "ZoIGVYnV7GRR"
      },
      "outputs": [],
      "source": [
        "# Define training constants\n",
        "batch_size: int = 64\n",
        "num_epochs: int = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-28T12:46:40.292657Z",
          "start_time": "2024-05-28T12:46:40.282719Z"
        },
        "id": "tNU34wZK7GRR"
      },
      "outputs": [],
      "source": [
        "def pair_list_to_x_y(pair_list: List[Tuple[Tuple[np.ndarray, np.ndarray], int]]) -> Tuple[Tuple[np.ndarray, np.ndarray], np.ndarray]:\n",
        "    _x, _y = zip(*pair_list)\n",
        "    _x_1, _x_2 = zip(*_x)\n",
        "    return (np.array(_x_1), np.array(_x_2)), np.array(_y, dtype=np.float32, )\n",
        "\n",
        "# Obtener las listas de train y test\n",
        "x_train, y_train = pair_list_to_x_y(mapped_train)\n",
        "x_val, y_val = pair_list_to_x_y(mapped_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-28T12:46:40.601343Z",
          "start_time": "2024-05-28T12:46:40.591404Z"
        },
        "id": "XElxzMpa7GRR"
      },
      "outputs": [],
      "source": [
        "# Preparar los conjuntos de datos de entrenamiento y validación\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=len(x_train)).batch(batch_size)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "val_dataset = val_dataset.batch(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-28T12:46:40.762267Z",
          "start_time": "2024-05-28T12:46:40.759596Z"
        },
        "id": "zDJK7CpB9xiw"
      },
      "outputs": [],
      "source": [
        "# Show shapes\n",
        "x_train[0].shape, x_train[1].shape, y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlUGxGn09xiw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-28T12:46:40.960340Z",
          "start_time": "2024-05-28T12:46:40.922757Z"
        },
        "id": "VKDzX1fJ9xiw"
      },
      "outputs": [],
      "source": [
        "# Construir y compilar el modelo\n",
        "model = build_and_compile_model()\n",
        "#tf.keras.utils.plot_model(model, show_shapes=True, show_layer_activations=True, )\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-28T12:46:43.103664Z",
          "start_time": "2024-05-28T12:46:41.094851Z"
        },
        "id": "U2krsMjT7GRR"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "model.fit(train_dataset, epochs=num_epochs, validation_data=val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-28T12:46:43.106894Z",
          "start_time": "2024-05-28T12:46:43.104650Z"
        },
        "id": "ndZy32667GRR"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import pearsonr\n",
        "x_test, y_test = pair_list_to_x_y(mapped_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-28T12:46:43.125798Z",
          "start_time": "2024-05-28T12:46:43.107428Z"
        },
        "id": "b2vGHeNg7GRR"
      },
      "outputs": [],
      "source": [
        "# Baseline\n",
        "def compute_pearson_baseline(x_, y_):\n",
        "    y_pred_baseline = []\n",
        "    for v1, v2 in zip(*x_):\n",
        "        d = 1.0 - spatial.distance.cosine(v1, v2)\n",
        "        y_pred_baseline.append(d)\n",
        "    # Calcular la correlación de Pearson entre las predicciones y los datos de prueba\n",
        "    correlation, _ = pearsonr(y_pred_baseline, y_.flatten())\n",
        "    return correlation\n",
        "# Imprimir el coeficiente de correlación de Pearson\n",
        "print(f\"Correlación de Pearson (baseline-train): {compute_pearson_baseline(x_train, y_train)}\")\n",
        "print(f\"Correlación de Pearson (baseline-validation): {compute_pearson_baseline(x_val, y_val)}\")\n",
        "print(f\"Correlación de Pearson (baseline-test): {compute_pearson_baseline(x_test, y_test)}\")\n",
        "with open(f'/content/drive/MyDrive/2n/PLH/PLH-4/resultats_{batch_size}_{num_epochs}_baseline.txt', 'w', encoding='utf-8') as file_opened:\n",
        "  file_opened.write('Correlación de Pearson (baseline-train):\\n')\n",
        "  file_opened.write(str(compute_pearson_baseline(x_train, y_train)) + '\\n')\n",
        "  file_opened.write('Correlación de Pearson (baseline-validation):\\n')\n",
        "  file_opened.write(str(compute_pearson_baseline(x_val, y_val)) + '\\n')\n",
        "  file_opened.write('Correlación de Pearson (baseline-test):\\n')\n",
        "  file_opened.write(str(compute_pearson_baseline(x_test, y_test)) + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-28T12:46:43.294476Z",
          "start_time": "2024-05-28T12:46:43.129327Z"
        },
        "id": "Qeq6GdH77GRR"
      },
      "outputs": [],
      "source": [
        "def compute_pearson(x_, y_):\n",
        "    # Obtener las predicciones del modelo para los datos de prueba. En este ejemplo vamos a utilizar el corpus de training.\n",
        "    y_pred = model.predict(x_)\n",
        "    # Calcular la correlación de Pearson entre las predicciones y los datos de prueba\n",
        "    correlation, _ = pearsonr(y_pred.flatten(), y_.flatten())\n",
        "    return correlation\n",
        "# Imprimir el coeficiente de correlación de Pearson\n",
        "print(f\"Correlación de Pearson (train): {compute_pearson(x_train, y_train)}\")\n",
        "print(f\"Correlación de Pearson (validation): {compute_pearson(x_val, y_val)}\")\n",
        "print(f\"Correlación de Pearson (test): {compute_pearson(x_test, y_test)}\")\n",
        "with open(f'/content/drive/MyDrive/2n/PLH/PLH-4/resultats_{batch_size}_{num_epochs}.txt', 'w', encoding='utf-8') as file_opened:\n",
        "  file_opened.write('Correlación de Pearson (train):\\n')\n",
        "  file_opened.write(str(compute_pearson(x_train, y_train)) + '\\n')\n",
        "  file_opened.write('Correlación de Pearson (validation):\\n')\n",
        "  file_opened.write(str(compute_pearson(x_val, y_val)) + '\\n')\n",
        "  file_opened.write('Correlación de Pearson (test):\\n')\n",
        "  file_opened.write(str(compute_pearson(x_test, y_test)) + '\\n')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}